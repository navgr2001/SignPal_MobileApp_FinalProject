I'm thrilled to share my final year research project: a mobile application for real-time Sri Lankan Sign Language (SLSL) recognition using a CNN-LSTM deep learning model. This project aims to support the deaf and hard-of-hearing community in Sri Lanka by enabling gesture recognition through a smartphone. The system captures 30-frame gesture sequences using MediaPipe to extract hand, face, and pose landmarks, which are then classified by an LSTM-based model trained on custom SLSL data. The app runs entirely offline, making it suitable for rural and low-connectivity environments, and was built using Python and Kivy for seamless integration with the AI backend. Technically, it contributes a lightweight, efficient architecture for mobile-based action recognition, while socially, it enhances accessibility through localized language support. I'm excited about how this work can evolve into two-way translation and avatar-based sign rendering in the future!
